services:

  llm: &llm
    image: ollama/ollama:latest
    profiles: ["linux"]
    # DON'T export this port to the host, else the healthcheck will fail -> though we can need it for debugging
    ports:
      - 11434:11434
    networks:
      - net

  llm-gpu:
    <<: *llm
    profiles: ["linux-gpu"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      # test: ["CMD-SHELL", "curl", "-f", "http://host.docker.internal:11434/"]
      test: ["CMD-SHELL", "curl", "-f", "http://llm-gpu:11434/"]
      interval: 30s
      timeout: 60s
      retries: 5
      start_period: 80s  

  postgres:
    # image: ankane/pgvector
    image: paradedb/paradedb:latest  # this image contains more things, including pgvector, pg_sparse and pg_bm25
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_HOST_AUTH_METHOD: "trust"
    ports:
      - 5432:5432
    volumes:
      - db_data:/var/lib/postgresql/data
      # - ./db_migrations/001_init.sql:/docker-entrypoint-initdb.d/init.sql  # this is not being run correctly
    networks:
      - net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready", "--host", "localhost",  "-d", "${POSTGRES_DB}", "-U", "${POSTGRES_USER}"]
      interval: 30s
      timeout: 60s
      retries: 5
      start_period: 80s  

  pull-model:
    image: genai-stack/pull-model:latest
    build:
      context: .
      dockerfile: pull_model.Dockerfile
    restart: on-failure
    environment:
      # - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434} 
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://llm-gpu:11434}  ## This is now working but we'll have to change the [llm|llm-gpu] depending the HW
      - LLM=${LLM-llama2}
    depends_on:
      llm-gpu:
        condition: service_started
    networks:
      - net
    tty: true


  bot:
    build:
      context: .
      dockerfile: bot.Dockerfile
    volumes:
      - $PWD/embedding_model:/embedding_model
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://llm-gpu:11434}
      - LLM=${LLM-llama2}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}
    networks:
      - net
    depends_on:
      - postgres
      # postgres:
      #   condition: service_started
        # condition: service_healthy
      - pull-model
      # pull-model:
      #   condition: service_completed_successfully
    x-develop:
      watch:
        - action: rebuild
          path: .
    ports:
      - 8501:8501


networks:
  net:

volumes:
  db_data:
